name: Deploy Preprocess Power Level Lambda

on:
  # Trigger the workflow on push
  push:
    branches: [ "main", "development" ] # push events on main branch
    paths:
      - "preprocess-power-level/**"
      - "preprocess_power_level/**"
      - "services/**"
      - "libs/**"
      - "requirements.txt"

env:
  PYTHON_VERSION: "3.12"
  BASE_FOLDER_NAME: "preprocess_power_level"
  S3_BUCKET_NAME: "rift-rewind-chrisbryann"
  LAMBDA_FUNCTION_NAME: "preprocess-power-level"
  DOCKERFILE_NAME: "Dockerfile.preprocess_power_level"
  LAYER_NAME: "riftrewind-deps" # << new

# the job defines a series of steps that execute on the same runner
jobs:
  CI:
    # Define the runner used in the workflow
    runs-on: ubuntu-22.04
    steps:
      # Check out repo so our workflow can access it
      - uses: actions/checkout@v4

      # Step-1 Setup Python
      - name: Set up Python
        # This action sets up a Python environment for use in actions
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # optional: architecture: x64 x64 or x86. Defaults to x64 if specified
      
      # Step-2 Install Python Virtual ENV
      - name: Install Python Virtual ENV
        run: pip3 install virtualenv
      
      # Step-3 Setup Virtual ENV
      # cache dependencies
      - name: Virtual ENV cache
        uses: actions/cache@v4
        id: cache-venv # name for referring later
        with:
          path: venv # what we cache: the virtual env
          # The cache key depends on requirements.txt
          key: ${{ runner.os }}-venv-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-venv-

      # Step-4 Build a Virtual ENV, but only if it doesn't already exists
      - name: Build venv and install deps
        if: steps.cache-venv.outputs.cache-hit != 'true'
        run: python -m venv venv && . venv/bin/activate && pip3 install -r requirements.txt

      # --- Function ZIP (kept as-is). Later you can drop vendored deps once layer is attached ---
      # - name: Create archive of dependencies (currently vendored into function zip)
      #   run: |
      #     cd ./venv/lib/python${{ env.PYTHON_VERSION }}/site-packages
      #     zip -r9 ../../../../${{ env.BASE_FOLDER_NAME }}.zip .

      - name: Add ${{ env.BASE_FOLDER_NAME }} files to Zip file
        run: zip -r ${{ env.BASE_FOLDER_NAME }}.zip ${{ env.BASE_FOLDER_NAME }} -x '**/__pycache__/*' '*.pyc'

      - name: Add /services files to Zip file
        run: zip -r ${{ env.BASE_FOLDER_NAME }}.zip services -x '**/__pycache__/*' '*.pyc'

      - name: Add /libs files to Zip file
        run: |
          if [ -d libs ]; then
            zip -r ${{ env.BASE_FOLDER_NAME }}.zip libs -x '**/__pycache__/*' '*.pyc'
          fi

      - name: Upload function zip artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.BASE_FOLDER_NAME }}
          path: ${{ env.BASE_FOLDER_NAME }}.zip

  # --- NEW: build a Lambda Layer with your Python deps (compatible with Lambda) ---
  BuildLayer:
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      - name: Build layer in Lambda base image (ensures binary compatibility)
        run: |
          docker run --rm \
            --entrypoint /bin/bash \
            -v "$PWD":/opt -w /opt \
            public.ecr.aws/lambda/python:${{ env.PYTHON_VERSION }} \
            -lc '
              rm -rf layer && mkdir -p layer/python
              # install only runtime deps (use requirements-layer.txt if you have one)
              pip install --no-cache-dir -r requirements.txt -t layer/python
              # prune junk to save MB
              find layer/python -type d -name "__pycache__" -prune -exec rm -rf {} +
              find layer/python -type d -name "tests" -prune -exec rm -rf {} +
              cd layer && zip -r ../layer.zip python
            '

      - name: Upload layer zip artifact
      # uploads artifacts from your workflow allowing you to share data between jobs
        # Stores data once a workflow is complete
        uses: actions/upload-artifact@v4
        with:
          name: lambda-layer
          path: layer.zip

  CD:
    runs-on: ubuntu-latest
    needs: [CI, BuildLayer]
    steps:
      - name: Install AWS CLI + jq
        uses: unfor19/install-aws-cli-action@v1
        with:
          version: 1
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY_SECRET }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Download Lambda function zip
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.BASE_FOLDER_NAME }}

      - name: Download Lambda layer zip
        uses: actions/download-artifact@v4
        with:
          name: lambda-layer

      # --- Publish/update the Layer and grab the version ARN ---
      - name: Publish Lambda Layer
        id: publish_layer
        run: |
          LAYER_OUT=$(aws lambda publish-layer-version \
            --layer-name "${{ env.LAYER_NAME }}" \
            --zip-file fileb://layer.zip \
            --compatible-runtimes python${{ env.PYTHON_VERSION }} \
            --compatible-architectures x86_64 arm64 \
            --region ${{ secrets.AWS_DEFAULT_REGION }})
          echo "$LAYER_OUT" | jq -r '.LayerVersionArn'
          echo "LAYER_ARN=$(echo "$LAYER_OUT" | jq -r .LayerVersionArn)" >> $GITHUB_ENV
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

      # --- Upload function code to S3 and update function code (your existing steps) ---
      - name: Upload to S3
        run: aws s3 cp ${{ env.BASE_FOLDER_NAME }}.zip s3://${{ env.S3_BUCKET_NAME }}/${{ env.BASE_FOLDER_NAME }}.zip
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Update function code
        run: |
          aws lambda update-function-code \
            --function-name ${{ env.LAMBDA_FUNCTION_NAME }} \
            --s3-bucket ${{ env.S3_BUCKET_NAME }} \
            --s3-key ${{ env.BASE_FOLDER_NAME }}.zip
          aws lambda wait function-updated --function-name ${{ env.LAMBDA_FUNCTION_NAME }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

      # --- Attach the freshly published Layer version to the function ---
      - name: Attach Layer to function
        run: |
          aws lambda update-function-configuration \
            --function-name ${{ env.LAMBDA_FUNCTION_NAME }} \
            --layers "$LAYER_ARN"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

      - name: Update Lambda environment variables
        run: |
          aws lambda update-function-configuration \
            --function-name ${{ env.LAMBDA_FUNCTION_NAME }} \
            --environment "Variables={DB_ARN=${{ secrets.DB_ARN }},SECRET_ARN=${{ secrets.DB_SECRET_ARN }},DB_NAME=${{ secrets.DB_NAME }},RIOT_API_KEY=${{ secrets.RIOT_API_KEY }},ENV=${{ secrets.ENV }}}"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
